{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetNet Model\n",
    "\n",
    "> Implementation of the parts of the metnet arch from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to implement the parts of the model [MetNet](https://arxiv.org/abs/2003.12140) from \"MetNet: A Neural Weather Model for Precipitation Forecasting\"\n",
    "\n",
    "![metenet_scheme](images/metnet_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from metnet_pytorch.layers import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the paper, the downsampler blocks are a bunch of convs and maxpooling layers, with out anything fancy, not even activations. From the paper: \n",
    "> MetNet aims at fully capturing the spatial context in the input patch. A trade-off arises between the\n",
    "fidelity of the representation and the memory and computation required to compute it. To maintain\n",
    "viable memory and computation requirements, the first part of MetNet contracts the input tensor\n",
    "spatially using a series of convolution and pooling layers. The t slices along the time dimension of\n",
    "the input patch are processed separately. Each slice is first packaged into an input tensor of spatial\n",
    "dimensions 256 × 256 (see Appendix A for the exact pre-processing operations). Each slice is then\n",
    "processed by the following neural network layers: a 3 × 3 convolution with 160 channels, a 2 × 2\n",
    "max-pooling layer with stride 2, three more 3 × 3 convolutions with 256 channels and one more\n",
    "2 × 2 max pooling layer with stride 2. These operations produce t tensors of spatial dimensions\n",
    "64 × 64 and 256 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DownSampler(in_channels):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, 160, 3, padding=1),\n",
    "                         nn.MaxPool2d((2,2), stride=2),\n",
    "                         nn.BatchNorm2d(160),\n",
    "                         nn.Conv2d(160, 256, 3, padding=1),\n",
    "                         nn.BatchNorm2d(256),\n",
    "                         nn.Conv2d(256, 256, 3, padding=1),\n",
    "                         nn.BatchNorm2d(256),\n",
    "                         nn.Conv2d(256, 256, 3, padding=1),\n",
    "                         nn.MaxPool2d((2,2), stride=2)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put less convs and added `nn.BatchNorm2d`, as I finally ended up using another image encoder, you can choose anything form torchvision or [timm](https://github.com/rwightman/pytorch-image-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DownSampler(3)\n",
    "test_eq(ds(torch.rand(2, 3, 256, 256)).shape,[2, 256, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can check, it divides by four the spatial resolution, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of MetNet encodes the input patch along the temporal dimension.  The spatially contracted slices are given to a recurrent neural network following the order of time.   We use a Convolutional Long Short-Term Memory network with kernel size 3×3 and 384 channels for the temporal encoding   (Xingjian et al., 2015). \n",
    "\n",
    "The result is a single tensor of size 64×64 and 384 channels, where each location summarizes spatially and temporally one region of the large contextin the input patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TemporalEncoder(Module):\n",
    "    def __init__(self, in_channels, out_channels=384, ks=3, n_layers=1):\n",
    "        self.rnn = ConvGRU(in_channels, out_channels, (ks, ks), n_layers, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        x, h = self.rnn(x)\n",
    "        return (x, h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TemporalEncoder(4, 8, n_layers=1)\n",
    "x,h = te(torch.rand(2, 10, 4, 12, 12))\n",
    "test_eq(h.shape, [2,8,12,12])\n",
    "test_eq(x.shape, [2,10,8,12,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning on Target Lead Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The leadtime is represented as an integeri= (Ty/2)−1indicating minutes from 2 to 480.  The integeriis tiled along thew×hlocations in the patch and is represented as an all-zero vector with a 1at positioniin the vector.  By changing the target lead time given as input, one can use the sameMetNet model to make forecasts for the entire range of target times that MetNet is trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=5\n",
    "i=3\n",
    "times = (torch.eye(seq_len)[i-1]).float().unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 1]), torch.Size([1, 2, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times.shape, ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = times * ones\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def condition_time(x, i=0, size=(12, 16), seq_len=15):\n",
    "    \"create one hot encoded time image-layers, i in [1, seq_len]\"\n",
    "    assert i<seq_len\n",
    "    times = (torch.eye(seq_len, dtype=x.dtype, device=x.device)[i]).unsqueeze(-1).unsqueeze(-1)\n",
    "    ones = torch.ones(1,*size, dtype=x.dtype, device=x.device)\n",
    "    return times * ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware, from `i=0` to `i=seq_len-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 12, 16]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3,5,2,8,8)\n",
    "i = 13\n",
    "ct = condition_time(x, i, (12,16), seq_len=15)\n",
    "assert ct[i, :,:].sum() == 12*16  #full of ones\n",
    "ct.shape, ct[:, 0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConditionTime(Module):\n",
    "    \"Condition Time on a stack of images, adds `horizon` channels to image\"\n",
    "    def __init__(self, horizon, ch_dim=2): \n",
    "        self.horizon = horizon\n",
    "        self.ch_dim = ch_dim\n",
    "        \n",
    "    def forward(self, x, fstep=0):\n",
    "        \"x stack of images, fsteps\"\n",
    "        bs, seq_len, ch, h, w = x.shape\n",
    "        ct = condition_time(x, fstep, (h,w), seq_len=self.horizon).repeat(bs, seq_len, 1,1,1)\n",
    "        x = torch.cat([x,ct], dim=self.ch_dim)\n",
    "        assert x.shape[self.ch_dim] == (ch + self.horizon)  #check if it makes sense\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ConditionTime(3)\n",
    "x = torch.rand(1,5,2,4,4)\n",
    "y = ct(x, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def feat2image(x, target_size=(128,128)): \n",
    "    \"This idea comes from MetNet\"\n",
    "    x = x.transpose(1,2)\n",
    "    return x.unsqueeze(-1).unsqueeze(-1) * x.new_ones(1,1,1,*target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 4, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,4,10)\n",
    "feat2image(x, target_size=(16,16)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To make MetNet’s receptive field cover the full global spatial context in the input patch, the third\n",
    "part of MetNet uses a series of eight axial self-attention blocks (Ho et al., 2019; Donahue and Si-\n",
    "monyan, 2019). Four axial self-attention blocks operating along the width and four blocks operating\n",
    "along the height are interleaved and have 2048 channels and 16 attention heads each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please install using pip:\n",
    "```bash\n",
    "pip install axial_attention\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from axial_attention import AxialAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = AxialAttention(\n",
    "    dim = 16,           # embedding dimension\n",
    "    dim_index = 1,       # where is the embedding dimension\n",
    "    heads = 8,           # number of heads for multi-head attention\n",
    "    num_dimensions = 2,  # number of axial dimensions (images is 2, video is 3, or more)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 16, 64, 64)\n",
    "test_eq(attn(x).shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model MetNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a small model to try the concept first.\n",
    "- The model will output all timesteps up to `horizon`.\n",
    "- We can condition on time before passing the images or after (saving some computations)\n",
    "- To start, we will output a timeseries, so we will put a `head` that generates one value per timestep. If you don't put any `head` you get the full attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MetNet(Module):\n",
    "    def __init__(self, image_encoder, hidden_dim, ks=3, n_layers=1, n_att_layers=1,\n",
    "                 head=None, horizon=3, n_feats=0, p=0.2, debug=False):\n",
    "        self.horizon = horizon\n",
    "        self.n_feats = n_feats\n",
    "        self.drop = nn.Dropout(p)\n",
    "        nf = 256  #from the simple image encoder\n",
    "        self.image_encoder = TimeDistributed(image_encoder)\n",
    "        self.ct = ConditionTime(horizon)\n",
    "        self.temporal_enc = TemporalEncoder(nf,  hidden_dim, ks=ks, n_layers=n_layers)\n",
    "        self.temporal_agg = nn.Sequential(*[AxialAttention(dim=hidden_dim, dim_index=1, heads=8, num_dimensions=2) for _ in range(n_att_layers)])\n",
    "        \n",
    "        if head is None:\n",
    "            self.head = Noop()\n",
    "        else:\n",
    "            self.head = head\n",
    "        \n",
    "        self.debug = debug\n",
    "        \n",
    "    def encode_timestep(self, x, fstep=1):\n",
    "        if self.debug:  print(f'Encode Timestep:(i={fstep})')\n",
    "        if self.debug:  print(f' input shape: {x.shape}')\n",
    "        \n",
    "        #Condition Time\n",
    "        x = self.ct(x, fstep)\n",
    "        if self.debug: print(f' CondTime->x.shape: {x.shape}')\n",
    "\n",
    "        ##CNN\n",
    "        x = self.image_encoder(x)\n",
    "        if self.debug:  print(f' encoded images shape: {x.shape}')\n",
    "        \n",
    "        #Temporal Encoder\n",
    "        _, state = self.temporal_enc(self.drop(x))\n",
    "        if self.debug:  print(f' temp_enc out shape: {state.shape}')\n",
    "        return self.temporal_agg(state)\n",
    "        \n",
    "            \n",
    "    def forward(self, imgs, feats):\n",
    "        \"\"\"It takes a rank 5 tensor \n",
    "        - imgs [bs, seq_len, channels, h, w]\n",
    "        - feats [bs, n_feats, seq_len]\"\"\"\n",
    "        if self.debug:  print(f' Input -> (imgs: {imgs.shape}, feats: {feats.shape})')    \n",
    "        #stack feature as images\n",
    "        if self.n_feats>0: \n",
    "            feats = feat2image(feats, target_size=imgs.shape[-2:])\n",
    "            imgs = torch.cat([imgs, feats], dim=2)\n",
    "        if self.debug:  print(f' augmented imgs:   {imgs.shape}')\n",
    "        \n",
    "        #Compute all timesteps, probably can be parallelized\n",
    "        res = []\n",
    "        for i in range(self.horizon):\n",
    "            x_i = self.encode_timestep(imgs, i)\n",
    "            out = self.head(x_i)\n",
    "            res.append(out)\n",
    "        res = torch.stack(res, dim=1).squeeze()\n",
    "        if self.debug: print(f'{res.shape=}')\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The params are as following:\n",
    "- `image_encoder`: A image 2 feature model, can be a VGG for instance.\n",
    "- `hidden_dim`: The channels on the temporal encoder ConvGRU cell.\n",
    "- `ks`: kernel size on the ConvGRU cell.\n",
    "- `n_layers`: Number of ConvGRU cells.\n",
    "- `n_att_layers`: Number of AxialAttention layers on the Temporal Aggregator.\n",
    "- `ct_first`: If we condition time before or after image encoding.\n",
    "- `head`: The head output of the model.\n",
    "- `horizon`: How many timesteps to predict.\n",
    "- `n_feats`: How many features are we passing to the model besides images, they will be encoded as image layers. See appendix of paper.\n",
    "- `p`: Dropout on temporal encoder.\n",
    "- `debug`: If True, prints every intermediary step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is structured with a `encode_timestep` method to condition on each timestep the input images:\n",
    "- First we take the input image sequence and condition on lead time\n",
    "- We pass this augmented image trhough the image_encoder\n",
    "- We apply the temporal encoder and \n",
    "- Finally we do the spatial attention.\n",
    "\n",
    "In the forward method:\n",
    "- We encode the numerical features on image channels using `feat2image`\n",
    "- We stack these with the original image\n",
    "- We iteratively call the `encode_timestep` and finally we return the predicted vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 5\n",
    "n_feats = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `image_encoder` must take 3 (RGB image) + horizon (for the conditining time) + feats (for the extra data planes added to image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = DownSampler(3+horizon+n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metnet = MetNet(image_encoder, hidden_dim=128, \n",
    "                ks=3, n_layers=1, horizon=horizon, \n",
    "                head=create_head(128, 1), n_feats=n_feats, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timeseries data, could be other thing that is sequential as the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = torch.rand(2, n_feats, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.rand(2, 10, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input -> (imgs: torch.Size([2, 10, 3, 128, 128]), feats: torch.Size([2, 4, 10]))\n",
      " augmented imgs:   torch.Size([2, 10, 7, 128, 128])\n",
      "Encode Timestep:(i=0)\n",
      " input shape: torch.Size([2, 10, 7, 128, 128])\n",
      " CondTime->x.shape: torch.Size([2, 10, 12, 128, 128])\n",
      " encoded images shape: torch.Size([2, 10, 256, 32, 32])\n",
      " temp_enc out shape: torch.Size([2, 128, 32, 32])\n",
      "Encode Timestep:(i=1)\n",
      " input shape: torch.Size([2, 10, 7, 128, 128])\n",
      " CondTime->x.shape: torch.Size([2, 10, 12, 128, 128])\n",
      " encoded images shape: torch.Size([2, 10, 256, 32, 32])\n",
      " temp_enc out shape: torch.Size([2, 128, 32, 32])\n",
      "Encode Timestep:(i=2)\n",
      " input shape: torch.Size([2, 10, 7, 128, 128])\n",
      " CondTime->x.shape: torch.Size([2, 10, 12, 128, 128])\n",
      " encoded images shape: torch.Size([2, 10, 256, 32, 32])\n",
      " temp_enc out shape: torch.Size([2, 128, 32, 32])\n",
      "Encode Timestep:(i=3)\n",
      " input shape: torch.Size([2, 10, 7, 128, 128])\n",
      " CondTime->x.shape: torch.Size([2, 10, 12, 128, 128])\n",
      " encoded images shape: torch.Size([2, 10, 256, 32, 32])\n",
      " temp_enc out shape: torch.Size([2, 128, 32, 32])\n",
      "Encode Timestep:(i=4)\n",
      " input shape: torch.Size([2, 10, 7, 128, 128])\n",
      " CondTime->x.shape: torch.Size([2, 10, 12, 128, 128])\n",
      " encoded images shape: torch.Size([2, 10, 256, 32, 32])\n",
      " temp_enc out shape: torch.Size([2, 128, 32, 32])\n",
      "res.shape=torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = metnet(imgs, feats)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def metnet_splitter(m):\n",
    "    \"A simple param splitter for MetNet\"\n",
    "    return [params(m.image_encoder), params(m.te)+params(m.head)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_layers.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
